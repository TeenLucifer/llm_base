# dsa_reproduce DeepSeek Sparse Attention (DSA) å¤ç°

æœ¬é¡¹ç›®åŸºäº Qwen2.5-0.5B è¯­è¨€æ¨¡å‹å¤ç° DSAï¼Œæ¨¡å‹å±‚é¢ä¿®æ”¹ + è®­ç»ƒ + è¯„ä¼°ã€‚

## ğŸ¯ é¡¹ç›®æ¦‚è¿°
å¤ç° DSAï¼ˆDeepSeek Sparse Attentionï¼‰ï¼šä»¥ Qwen2.5-0.5B ä¸ºåŸºåº§ï¼Œä½¿ç”¨ deepctrl-sft-data æ•°æ®é›†å¾®è°ƒï¼Œå¯¹æ¯”å¼•å…¥ DSA ç»“æ„åçš„æ•ˆæœã€‚

## ğŸ” ç®—æ³•åŸç†

DSA çš„åŸç†éœ€è¦ä»æ¨¡å‹å’Œè®­ç»ƒä¸¤ä¸ªå±‚é¢è¿›è¡Œè¯´æ˜ã€‚

### æ¨¡å‹å±‚é¢
å‚è€ƒ DeepSeek æŠ€æœ¯æŠ¥å‘Šä¸­çš„ç¤ºæ„å›¾ï¼ŒDSA åŒ…æ‹¬è½»é‡æ‰“åˆ†å™¨ï¼ˆlightning indexerï¼‰å’Œ top-k é€‰æ‹©å™¨ï¼ˆtop-k selectorï¼‰ä¸¤éƒ¨åˆ†ã€‚

DSA çš„æ•´ä½“æµç¨‹æ˜¯å…ˆç”¨ lightning indexer å¯¹ token è¿›è¡Œä¸€æ¬¡æ‰“åˆ†ï¼Œä»…é€‰æ‹©é‡è¦æ€§å¾—åˆ† top-k çš„ token è¿›è¡Œ attention è®¡ç®—ï¼Œè¿™æ ·å°±å®ç°äº†æ³¨æ„åŠ›è®¡ç®—çš„ç¨€ç– Sparseã€‚

e.g. è¾“å…¥æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ä¸º 128Kï¼Œä»…é€‰æ‹© top-2048 çš„ token è¿›è¡Œ attention è®¡ç®—ï¼Œå®ç°äº†æ¨ç†æ—¶çš„æè‡´æ•ˆç‡ï¼Œæ¨ç†æˆæœ¬ä» 128K token -> 2048 tokenã€‚
<p align="center">
    <img src="./docs/dsa_framework.png">
</p>

è®¾è®¡ä¸Š DSA å±äºä¸€ä¸ªç½‘ç»œç»“æ„ï¼Œçº¿æ€§æ˜ å°„å‡½æ•° w åŠ ä¸Šæ¿€æ´»å‡½æ•° ReLUã€‚
<p align="center">
    <img src="./docs/dsa_form.png">
</p>

ä¸‹é¢æ˜¯æ¨¡å‹ç»“æ„ä¸Šçš„ä»£ç å®ç°
```python
class Indexer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.hidden_size: int = config.hidden_size
        self.n_heads: int = config.num_attention_heads
        self.key_value_heads = config.num_key_value_heads
        self.head_dim: int = config.hidden_size // config.num_attention_heads
        self.index_topk: int = 128

        self.wk = nn.Linear(self.hidden_size, self.head_dim) 
        self.weights_proj = nn.Linear(self.hidden_size, self.n_heads)

        self.register_buffer("k_cache", None, persistent=False)

    def forward(self, hidden_states: torch.Tensor, query_states: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor, mask=None):
        bsz, seqlen, _ = hidden_states.size()
        key_states = self.wk(hidden_states)

        weights = self.weights_proj(hidden_states) * self.n_heads ** -0.5 # [bs, seqlen, n_heads]

        # q:[bs, n_heads, seqlen, head_dim]
        # k:[bs, seqlen, head_dim]

        if seqlen > 1:
            self.k_cache = key_states

        if seqlen == 1:
            key_states = torch.cat([self.k_cache, key_states], dim=1) # [bs, seqlen, head_dim]
            self.k_cache = key_states

        key_states = key_states.unsqueeze(1) # [bs, 1, seqlen, head_dim]
        key_states, key_states = apply_rotary_pos_emb(key_states, key_states, cos, sin)

        # [bs, n_heads, seqlen, head_dim] * [bs, 1, head_dim, seqlen] --> [bs, n_heads, seqlen, seqlen]
        attn_scores = query_states @ key_states.transpose(2,3)
        attn_scores = F.relu(attn_scores, inplace=False)

        # [bs, n_heads, seqlen, 1] * [bs, n_heads, seqlen, seqlen] --> [bs, n_heads, seqlen, seqlen]
        attn_scores = weights.transpose(1,2).unsqueeze(-1) * attn_scores

        attn_scores = attn_scores.sum(1, keepdim=True) # [bs, 1, seqlen, seqlen]

        if mask is not None:
            attn_scores = attn_scores + mask

        topk_indices = attn_scores.topk(min(self.index_topk, key_states.shape[2]), dim=-1)[1]
        return topk_indices, attn_scores
```

### è®­ç»ƒå±‚é¢
è®­ç»ƒå±‚é¢ä¸Š DeepSeek é‡‡ç”¨äº†çš„é˜¶æ®µè®­ç»ƒæ–¹å¼ï¼Œä¸€é˜¶æ®µ warmup + äºŒé˜¶æ®µè”åˆè®­ç»ƒã€‚å®˜æ–¹æŠ€æœ¯æŠ¥å‘Šä¸­å¯¹è¿™éƒ¨åˆ†è®­ç»ƒæè¿°è¾ƒä¸ºæ¨¡ç³Šï¼Œä¸‹é¢ç»“åˆGè€å¸ˆçš„åˆ†æè¯¦ç»†è®²è§£ä¸€ä¸‹ã€‚

1. warmup é˜¶æ®µï¼šè®­ç»ƒæ‰“åˆ†å™¨ï¼Œè®© indexer èƒ½å¤ŸåŒºåˆ† token çš„é‡è¦æ€§

å†»ç»“åŸºæ¨¡å‚æ•°ï¼Œä»…è®­ç»ƒæ‰“åˆ†å™¨ï¼Œé‡‡æ ·è’¸é¦çš„æ€è·¯ã€‚dense attention ä¸º teacherï¼Œindexer ä¸º studentã€‚dense attention è¾“å‡ºçš„å¤šå¤´æ³¨æ„åŠ›ç›¸åŠ ï¼Œå†ç”¨ L1 å½’ä¸€åŒ–å¾—åˆ° token çš„é‡è¦æ€§åˆ†å¸ƒï¼Œç”¨ KL æ•£åº¦çº¦æŸ indexer å»é€¼è¿‘è¿™ä¸ªé‡è¦æ€§åˆ†å¸ƒã€‚è¿™ä¸€æ­¥ä¸­ indexer ç®—å‡ºæ¥çš„å…·ä½“æ•°å€¼æ˜¯å¤šå°‘ä¸é‡è¦ï¼Œèƒ½åŒºåˆ†å¤§å°å…³ç³»å³å¯ã€‚è€Œä¸”ç”±äº indexer å‚æ•°é‡å¾ˆå°‘ï¼Œè¿™ä¸ªæ­¥éª¤çš„è®¡ç®—æ¶ˆè€—è¿œå°äº dense attentionã€‚

ä¸‹é¢æ˜¯ warmup è®­ç»ƒçš„ä»£ç å®ç°
```python
# warmup è®­ç»ƒæŸå¤±å‡½æ•°
def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
    outputs = model(**inputs, output_attentions=True)
    all_attentions = outputs.attentions
    ce_loss = outputs.loss

    attention_kl_loss = torch.tensor(0.0, device=outputs.loss.device)

    for attention in all_attentions:
        topk_indices, raw_attn_weights, indexer_attn_scores = attention

        raw_attn_weights_topk = torch.gather(raw_attn_weights, -1, topk_indices.expand(-1, raw_attn_weights.shape[1], -1, -1))

        raw_attn_weights_topk = F.softmax(raw_attn_weights_topk, dim=-1)

        # headç»´åº¦æ±‚å’Œ
        raw_attn_weights_topk = raw_attn_weights_topk.sum(1, keepdim=True)

        # L1å½’ä¸€åŒ–
        raw_attn_weights_topk = raw_attn_weights_topk / torch.norm(raw_attn_weights_topk, dim=-1, p=1, keepdim=True)

        indexer_attn_scores_topk = torch.gather(indexer_attn_scores, -1, topk_indices)
        # [batch_size, 1, seq_len, seq_len]
        indexer_attn_scores_topk = F.softmax(indexer_attn_scores_topk, dim=-1)
        indexer_attn_scores_topk = torch.clamp(indexer_attn_scores_topk, min=1e-8)
        kl_loss = F.kl_div(indexer_attn_scores_topk.log(), raw_attn_weights_topk.detach())

        attention_kl_loss = attention_kl_loss + kl_loss

    attention_kl_loss = attention_kl_loss / len(all_attentions)

    loss = ce_loss + attention_kl_loss

    return (loss, outputs) if return_outputs else loss
```
2. è”åˆè®­ç»ƒé˜¶æ®µï¼šè®©åŸºæ¨¡é€‚åº”ç¨€ç–æ³¨æ„åŠ›ï¼Œå¹¶è®© indexer é€¼è¿‘ dense attention top-k token é‡è¦æ€§åˆ†å¸ƒ

å¼•å…¥ top-k selector å¹¶ä¸”åŸºæ¨¡å‚æ•°å’Œ indexer ä¸€èµ·è®­ç»ƒã€‚åŒæ ·é‡‡ç”¨è’¸é¦çš„æ€è·¯ï¼Œä½†æ­¤æ—¶æ˜¯è®© indexer çš„ top-k token é‡è¦æ€§åˆ†å¸ƒå»é€¼è¿‘ dense attention çš„ top-k token é‡è¦æ€§åˆ†å¸ƒï¼Œè¿™ä¸€æ­¥ä¸ warmup ä¸€è‡´ã€‚å¹¶ä¸”ç”±äºç¨€ç–æ³¨æ„åŠ›ä¸ç¨ å¯†æ³¨æ„åŠ›ä¸å®Œå…¨ä¸€è‡´ï¼Œå› æ­¤éœ€è¦è§£å†»åŸºæ¨¡å‚æ•°ï¼Œè®©åŸºæ¨¡é€‚åº”ç¨€ç–ç»“æ„ã€‚

ä¸‹é¢æ˜¯è”åˆè®­ç»ƒçš„ä»£ç å®ç°
```python
# ä¸‹é¢æ˜¯è”åˆè®­ç»ƒçš„æŸå¤±å‡½æ•°
def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
    outputs = model(**inputs, output_attentions=True)
    all_attentions = outputs.attentions
    ce_loss = outputs.loss

    attention_kl_loss = torch.tensor(0.0, device=outputs.loss.device)

    for attention in all_attentions:
        topk_indices, raw_attn_weights, indexer_attn_scores = attention

        raw_attn_weights_topk = torch.gather(raw_attn_weights, -1, topk_indices.expand(-1, raw_attn_weights.shape[1], -1, -1))

        raw_attn_weights_topk = F.softmax(raw_attn_weights_topk, dim=-1)

        # headç»´åº¦æ±‚å’Œ
        raw_attn_weights_topk = raw_attn_weights_topk.sum(1, keepdim=True)

        # L1å½’ä¸€åŒ–
        raw_attn_weights_topk = raw_attn_weights_topk / torch.norm(raw_attn_weights_topk, dim=-1, p=1, keepdim=True)

        indexer_attn_scores_topk = torch.gather(indexer_attn_scores, -1, topk_indices)
        # [batch_size, 1, seq_len, seq_len]
        indexer_attn_scores_topk = F.softmax(indexer_attn_scores_topk, dim=-1)
        indexer_attn_scores_topk = torch.clamp(indexer_attn_scores_topk, min=1e-8)
        kl_loss = F.kl_div(indexer_attn_scores_topk.log(), raw_attn_weights_topk.detach())

        attention_kl_loss = attention_kl_loss + kl_loss

    attention_kl_loss = attention_kl_loss / len(all_attentions)

    loss = ce_loss + attention_kl_loss

    return (loss, outputs) if return_outputs else loss
```

## ğŸ“š æ•°æ®é›†
æ•°æ®é›†é€‰ç”¨çš„æ˜¯åŒ æ•°ç§‘æŠ€çš„ deepctrl-sft-dataï¼ŒåŒ…å«10Mæ¡æ•°æ®çš„ä¸­æ–‡æ•°æ®é›†å’ŒåŒ…å«2Mæ¡æ•°æ®çš„è‹±æ–‡æ•°æ®é›†ã€‚

å®é™…åšå®éªŒéªŒè¯çš„æ—¶å€™æŒ‘å‡º 10000~20000 æ¡å¤§äº 1024 token çš„å°±å¯ä»¥äº†ï¼Œä¸éœ€è¦è·‘å®Œæ•´çš„æ•°æ®é›†ï¼Œä¸€åƒå¤šä¸‡æ¡å¤ªå¤šäº†ã€‚

æ•°æ®é›†ç¤ºä¾‹å¦‚ä¸‹

<p align="center">
    <img src="./docs/dataset_example.png">
</p>

## ğŸ“Š æ•ˆæœå±•ç¤º

### è¿è¡Œç¯å¢ƒ
- **step 1 warmupè®­ç»ƒ**: 0.5hï¼ˆ1 Ã— AutoDL vGPU-32GBï¼‰
- **step 2 è”åˆè®­ç»ƒ**: 3hï¼ˆ1 Ã— AutoDL vGPU-48GBï¼‰

### è®­ç»ƒæ•ˆæœ
<table>
  <tr>
    <td align="center">
      <img src="./docs/warmup_loss.png" width="90%"><br>
      <em>å›¾ 1ï¼šwarmup è®­ç»ƒè¿‡ç¨‹ä¸­çš„ loss æ›²çº¿</em>
    </td>
    <td align="center">
      <img src="./docs/train_loss.png" width="95%"><br>
      <em>å›¾ 2ï¼šè”åˆè®­ç»ƒè¿‡ç¨‹ä¸­çš„ loss æ›²çº¿</em>
    </td>
  </tr>
</table>
å›¾ 1 ä¸­ä»…è®­ç»ƒè½»é‡æ‰“åˆ†å™¨ï¼Œå› æ­¤æ€§èƒ½å¿«é€Ÿæ”¶æ•›ã€‚

å›¾ 2 ä¸­åŸºæ¨¡å‚æ•°ä¸è½»é‡æ‰“åˆ†å™¨è”åˆè®­ç»ƒï¼Œlosså€¼éšç€è®­ç»ƒçš„è¿›è¡ŒæŒç»­åœ¨æ‰ï¼Œè¯´æ˜ sparse attention çš„æ€§èƒ½åœ¨é€æ¸é€¼è¿‘ dense attentionã€‚

## ğŸš€ é¡¹ç›®éƒ¨ç½²è¿è¡Œ

### æ¨¡å‹å’Œæ•°æ®é›†ä¸‹è½½

```bash
# æ¨¡å‹æˆ–æ•°æ®æœ‰ç½‘ç»œé—®é¢˜å¯ä»¥åœ¨modelscopeæˆ–è€…hfé•œåƒç«™ä¸‹è½½

# ä¸‹è½½ Qwen2.5-0.5B æ¨¡å‹
git clone https://modelscope.cn/models/Qwen/Qwen2.5-0.5B

# deepctrl-sft-data æ•°æ®é›†
git clone https://modelscope.cn/datasets/deepctrl/deepctrl-sft-data
```

### ä¾èµ–å®‰è£…

```bash
pip install uv
uv sync
```

### è®­ç»ƒæ­¥éª¤

```bash
# è½¬æ¢æ•°æ®é›†
python data_preprocess.py

# step 1. warmup è®­ç»ƒ. å†»ç»“åŸºæ¨¡å‚æ•°, ä»…è®­ç»ƒ lightning indexer
python warmup_train.py

# step 2. è”åˆè®­ç»ƒ. åŸºæ¨¡ä¸ lightning indexer è”åˆå­¦ä¹ 
python train.py

# è¯„ä¼°è®­ç»ƒå‰åæ¨¡å‹ååé‡
python eval.py
```

## ğŸ ç¼ºé™·
ç”¨è®­ç»ƒå‰åæ¨¡å‹æ¨ç†çš„ååé‡å¯¹æ¯”æ¥çœ‹æ•ˆæœä¸æ˜æ˜¾ï¼Œæœ‰ä»¥ä¸‹ä¸¤æ–¹é¢åŸå› 

1. ç›®å‰é¡¹ç›®ä»…å®ç°äº† lightning indexer çš„è®­ç»ƒï¼Œè€Œæ¨ç†æ—¶å®é™…çš„ top-k ç¨€ç–æ³¨æ„åŠ›è®¡ç®—ä¾èµ–äº dense attentionï¼Œå¯¼è‡´å­˜åœ¨èµ„æºæ¶ˆè€—
    - ä¼˜åŒ–æ–¹æ¡ˆï¼šä¿®å¤æ¨ç†æ—¶çš„ top-k ç¨€ç–æ³¨æ„åŠ›è®¡ç®—ï¼Œä»…ä¾é  indexer é€‰å– top-k çš„ k v å€¼è¿›è¡ŒçŸ©é˜µä¹˜æ³•è¿ç®—
2. è®­ç»ƒå‰çš„æ¨¡å‹ç”¨ AutoModelForCausalLM åŠ è½½ï¼Œè®­ç»ƒåæ¨¡å‹ç”¨è‡ªå®šä¹‰çš„ç»“æ„åŠ è½½ï¼ŒçŸ©é˜µä¹˜æ³•çš„ç®—å­åº”ç”¨å­˜åœ¨å·®å¼‚ï¼Œå¯¼è‡´è®­ç»ƒå‰åæ¨¡å‹ç»Ÿè®¡æ¶ˆè€—å­˜åœ¨å·®å¼‚
    - ä¼˜åŒ–æ–¹æ¡ˆï¼šè‡ªå®šä¹‰ä¸€ä¸ªåŸºäº densen attention çš„ qwen2.5-0.5b æ¨¡å‹ç»“æ„ï¼Œç®—å­ä¸ sparse attention çš„æ¨¡å‹ç»“æ„ä¸€è‡´ï¼Œä»…ä¿æŒsparse attention å’Œ dense attention å·®å¼‚è¿›è¡Œæ¨ç†å¯¹æ¯”

ğŸ‘€ ä¸»åŒ…å¤ç°è¿™ä¸ªé¡¹ç›®çš„ä¸»è¦ä½œç”¨æ˜¯å­¦ä¹ å’Œäº†è§£ dsa çš„æœºåˆ¶ï¼Œè·Ÿè¸ª DeepSeek çš„çƒ­ç‚¹ä¿¡æ¯ï¼Œå­¦ä¹ å®Œè®­ç»ƒè¿‡ç¨‹åŸºæœ¬å·²ç»æŒæ¡ã€‚

ğŸ‘ ä¸Šè¿°æ¨ç†çš„é—®é¢˜å·²å®šä½åˆ°åŸå› ï¼Œä¿®å¤ç¼ºé™·å¯¹ä¸ªäººæ¥è¯´æ€§ä»·æ¯”å¤ªä½æš‚ä¸è°ƒæ•´ï¼Œæœ‰èƒ½åŠ›çš„åŒå­¦æ¬¢è¿æPRè¿›è¡Œä¼˜åŒ–è°ƒæ•´ã€‚

## ğŸ“– å‚è€ƒèµ„æ–™

1. æœ¬é¡¹ç›®åœ¨bç«™upä¸»[å·æ˜Ÿä¹æœˆ333](https://github.com/wyf3/llm_related)çš„åŸºç¡€ä¸ŠäºŒå¼€ï¼Œè¡¥å……äº†æµ‹è¯„å¯¹æ¯”

2. [DeepSeek V3.2 Technical Report](https://arxiv.org/pdf/2412.19437)ï¼ŒæŠ€æœ¯æŠ¥å‘Šä¸­æå‡ºäº†ä¸€ç§ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œæå¤§çš„èŠ‚çœäº†æ¨ç†çš„èµ„æºæ¶ˆè€—ï¼Œå°†æ€§ä»·æ¯”åšåˆ°äº†æè‡´ã€‚

## ğŸ¤ è´¡çŒ®ä¸äº¤æµ

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›é¡¹ç›®ã€‚è¿™æ˜¯ä¸€ä¸ªæ¢ç´¢æ€§çš„å­¦ä¹ é¡¹ç›®ï¼Œæ—¨åœ¨åˆ†äº«å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒå’Œéƒ¨ç½²çš„ç»éªŒã€‚

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨å¼€æºè®¸å¯è¯ï¼Œè¯¦è§LICENSEæ–‡ä»¶ã€‚

---

**æ³¨æ„**: è¿™æ˜¯ä¸€ä¸ªä»¥æ¢ç´¢å’Œå­¦ä¹ ä¸ºç›®æ ‡çš„é¡¹ç›®ï¼Œä»£ç å®ç°å¯èƒ½å­˜åœ¨ä¸è¶³ä¹‹å¤„ã€‚å¦‚æœæ‚¨å‘ç°ä»»ä½•é—®é¢˜ï¼Œæ¬¢è¿æå‡ºIssueæˆ–PRã€‚æ„Ÿè°¢æ‚¨çš„æŒ‡æ­£å’Œäº¤æµï¼
